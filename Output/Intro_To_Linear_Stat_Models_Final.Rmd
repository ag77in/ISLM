---
title: "Intro_To_Linear_Statistical_Models_Final"
author: "Aman Goswami & Abdulaziz Alshalfan"
date: "12/15/2020"
output: pdf_document
---

### General Information : Aziz and I collaborated through Github on the final given the timezone difference. We uploaded our codes at a repository with details here - https://github.com/ag77in/ISLM. We both did all 6 questions together for a dry run and then proceeded to discuss to see how we can improve the answers and the final results. We then jointly created this document.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Let us load libraries

```{r results='hide', message=FALSE, warning=FALSE}
# clear environment
rm(list = ls())

# defining libraries

library(ggplot2)
library(dplyr)
library(PerformanceAnalytics)
library(data.table)
library(sqldf)
library(nortest)
library(MASS)
library(rpart)
library(class)
library(ISLR)
library(scales)
library(ClustOfVar)
library(GGally)
library(reticulate)
library(ggthemes)
library(RColorBrewer) 
library(gridExtra)
library(kableExtra)
library(Hmisc) 
library(corrplot)
library(energy)
library(nnet)
library(Hotelling)
library(car)
library(devtools)
library(ggbiplot)
library(factoextra)
library(rgl)
library(FactoMineR)
library(psych)
library(nFactors)
library(scatterplot3d)
library(lmtest)
library(mctest)
library(aod)
library(InformationValue)
library(pROC)
library(tidyverse)
library(caret)
library(Information)
library(reshape)
library(olsrr)
library(faraway)
library(readxl)
library(tidyverse)
library(lubridate)
library(tsutils)
library(seastests)
library(emmeans)
library(forecast)
library(tseries)
library(tidyquant)
library(modelr)
library(grid)
library(aTSA)
library(fpp2) 
library(MLmetrics)
```

### 1) Fit a model to explain price in terms of the predictors. Perform regression diagnostics to answer the following questions. Display any plots that are relevant. Suggest improvements if any.

### Let us load the data and summarise the information

```{r}
# reading data
stockdata <- read.csv('/Users/mac/Downloads/final-2020-canvas/datasets/stockdata.csv')
str(stockdata)
```

```{r}
#summary
summary(stockdata)
```

### Key observations -
1. cap.to.gdp., trailing.pe, gaap, avg.allocation are marginally positively skewed while q.ratio is marginally negatively skewed. \
2. Volatility looks normal however we will confirm this later. \
3. We don't see any evidence of outliers but we will check the plots before commenting on this. \

### Any missing values ?

```{r}
data <-na.omit(stockdata)
str(data)
```

We did not find any missing values in the data.

### Correlation plot -

We check correlation before moving towards modeling exercise.

```{r}
M<-cor(stockdata)
head(round(M,2))
corrplot(M, method="color")
```

### Key observations -
1. We see price is correlated strongly with avg.allocation (+0.62). \
2. We also see price being correlated with q.ratio, cap.to.gdp, gaap and trailing.pe in similar range (+0.39-+0.42). \
3. We see very little correlation between most of the independent variables however we note the -ve correlations of cap.to.gdp with q.ratio, trailing.pe and gaap albeit small. \

### Outlier/ Univariate checks -

We confirm our earlier hypothesis of no outliers by looking at some univariate and outlier checks.

```{r}
par(mfrow=c(2,5))
for (i in 1:length(stockdata)) {
        boxplot(stockdata[,i], main=names(stockdata[i]), type="l")
}

```

We see no evidence of any outliers in the univariate form.

We now proceed to modeling exercise.

\newpage

### 1a) Fit a model to explain price in terms of the predictors. Which variables are important, can any of the variables be removed ? Please use F-tests to justify.

We use lm() function for this regression

```{r}
fit <- lm(price~cap.to.gdp+ q.ratio+gaap+trailing.pe+avg.allocation+vol, data=stockdata)
summary(fit)
```

We see significant result in p-value for all variables except for Volatility which is not significant even at 10% level of signifance hence this variable can be removed from the model. We get F value of 318 with probability <0.5 indicating the joint hypothesis of this model being better than null model. Every other variable seems important to prediction.

The above model has a good R-square of 95.35%.

We now confirm this with joint hypothesis test using F-statistic. We use both anova and  linearHypothesis() for this.

### Model without vol 

```{r}
fit2 <- lm(price~cap.to.gdp+ q.ratio+gaap+trailing.pe+avg.allocation, data=stockdata)
summary(fit2)
```

We now look at Anova
```{r}
anova(fit2, fit)
```

Since p-value of 0.5691 is > 0.5 we conclude that the model with vol is not significantly better than the model without vol.

We can see this with individual F tests for each variable using linearhypothesis() now as well.

```{r}
linearHypothesis(fit, c("cap.to.gdp=0"))
linearHypothesis(fit, c("q.ratio =0"))
linearHypothesis(fit, c("gaap=0"))
linearHypothesis(fit, c("trailing.pe=0"))
linearHypothesis(fit, c("avg.allocation=0"))
linearHypothesis(fit, c("vol=0"))
```

We ran the F-tests for the hypothesis of each individual variable and in only one case is the F-score is very low (0.3265) in the 'vol'=0 hypothesis. Hence, this confirms from our linear regression model again that given the Pr(>F) is 0.56, we find volatility to not influence the price and hence we can remove this variable from our model.

Sometimes, however it is important to run the heteroskedastic robust version of the F-test as well. We do this as follows -

```{r}
linearHypothesis(fit, c("cap.to.gdp=0"),white.adjust = "hc1")
linearHypothesis(fit, c("q.ratio =0"),white.adjust = "hc1")
linearHypothesis(fit, c("gaap=0"),white.adjust = "hc1")
linearHypothesis(fit, c("trailing.pe=0"),white.adjust = "hc1")
linearHypothesis(fit, c("avg.allocation=0"),white.adjust = "hc1")
linearHypothesis(fit, c("vol=0"),white.adjust = "hc1")
```

We see no change in results although the Pr(>F) for vol=0 hypothesis is slightly higher than before. We still only confirm that volatility can be removed from the model and state that fit2 is our best model for now.

### 1b) Check the constant variance assumption for the errors.

We do this in 3 ways - we look at scale-location plot of the regression, then perform ncvTest() and finally Breusch-pagan test for homoskedasticity.

```{r}
plot(fit,which=3)
```
\

We mostly see as they say "stars in the sky" expression in the above graph. While the line visually is not completely horizontal, we do not in particular see any pattern and it would be hard to deny that the errors are homoskedastic. However, we perform further tests to confirm our suspicion.

###  ncvTest() For Homoscedasticity

```{r}
ncvTest(fit)
```
\
We see a p-value > .05, indicating homoscedasticity.

### Breusch-Pagan Test For Homoscedasticity

```{r}
bptest(fit)
```

\
We once again see a p-value > .05, indicating homoscedasticity.

### 1c) Check the independentness of the errors assumption.

We can do this with the durbin watson statistic. The Durbin Watson examines whether the errors are autocorrelated  with themselves. The null states that they are not autocorrelated.

```{r}
durbinWatsonTest(fit)
```
\
We see that p-value > 0.05 so the errors are not autocorrelated.

### 1d) Check the normality assumption.

We again do this in 2 ways - we look at QQ plot and perform the Shapiro Wilk normality test.

The normal probability plot of residuals should approximately follow a straight line.

```{r}
plot(fit,which=2)
```
\
We see points falling mostly along reference line however we also see some falling outside on both sides of the quantile-spectrum so we dig deeper with a statistical test.

### Shapiro-Wilk Normality Test

```{r}
resid <- studres(fit) 
shapiro.test(resid)
```

From the p-value = 0.02474 < 0.05, we can see that the residuals are not normally distributed

### 1e) Is non-linearity a problem ?

The linearity assumption can be checked by inspecting the Residuals vs Fitted plot.

```{r}
plot(fit,which=1)
```
\

This is very interesting since the residuals take both +ve and -ve values. However, we see an inverted U shaped curve above. This suggests that the fit of the model can be improved by taking the square of some explanatory variable. 

### 1f) Check for outliers, compute and plot the cook's distance.

A standard way to check for outliers is to look at residuals above a certain threshold. An example would be as follows -

```{r}
rstandard(fit)[abs(rstandard(fit)) > 2]
```

Here, we see points 9, 29, 36, 42, 59 and 85 with large residuals but note that not all of them or maybe none of them could be outliers. So we now look at the model plot of Residuals vs leverage.

```{r}
plot(fit, which=5)
```

Leverage statistic is defined as - \
$\hat{L}= \dfrac{2(p+1)}{n}$ 
where $p$ is number of predictors and $n$ is number of observations \

In the above graph we see all points fall under the dashed lines of the cook's distance (missing) which tells us there are no outliers in the data but still some influential points do exist.

We can plot the cook's distance with the below command -

```{r}
#Cook's distance
plot(fit, 4)
```

We see that apart from three points - 9, 36 & 85, everyone else's cook's distance is below 0.06

We also compute the cook's distance for each observation as follows -

```{r}
cooks.distance(fit)
```
\

### 1g) Check for influential points.

This was partially done above itself, but neverthless we can check for 
influential points through the plot itself. 

```{r}
# High leverage points
plot(fit, which=5)
```

So from this plot again, we see points
9, 36, 85 as values of extreme nature and we see
these as influential points. We also see some other point to the extreme right 
with high leverage but low residual. 
Either ways, we check for more robust solution through below.

A rule of thumb is that an observation has high influence 
if Cook's distance exceeds $\dfrac{4}{(n - p - 1)}$

```{r}
cooks.distance(fit) > 4 / length(cooks.distance(fit))
```

We see points 9, 36, 42, 59, 70, 85 as influential points just going by cooks distance.

We also however check for hatvalues in the data.

```{r}
hatvalues(fit) > 0.1
```

We see only few observations with hatvalues above 0.1. These are points 9, 35, 36, 48, 68 & 94.

Now, we combine the above two results of cooks distance and hatvalues which is exactly what influence.measures does for us.

```{r}
summary(influence.measures(fit))
```

The last two columns give the cooks distance and the hatvalues. We see points 9,42,59 and 85 as influential observations given cooks distance > 0.5 and hat value also > 0.5.

We plot the results slightly better now -

```{r}
ols_plot_cooksd_bar(fit)
ols_plot_resid_stand(fit)
ols_plot_resid_lev(fit)
```

Above we plotted the cooks d bar plot, the standarized residual plot and the rstudent vs leverage plot.

The first plot tells us that points 9,85, 36, 59, 42 and 70 have larger cooks distance than other points
The second plot tells us that points 59, 85, 42, 9, 29 and 36 have larger residuals.
And the third plot tells us that if we use a leverage threshold of 0.14 we do not get any outlier but we see how close points 9, 36, 85, 42, 29 and 59 are in the range we create and these are clearly the most influential points in the data.

### 1h) The return at time t is defined as r(t)=[p(t+1)/p(t)]-1 where p is price data for day t. Are returns normally distributed ? Please justify using qq plot and normality test.

First, we create a 'return' variable in the data using the above expression.

```{r}
# create the lag of price
stockdata$price_lag  <- lag(stockdata$price)
stockdata$return <- (stockdata$price_lag/stockdata$price) - 1 

# We ignore the first observation as it is NA
return_data <- na.omit(stockdata)
```

We now plot the histogram of returns -

```{r}
hist(x = return_data$return,
     breaks = 20,
     probability = TRUE,
     main = "Histogram of Returns",
     xlab = "Returns")

lines(density(return_data$return))

```

The returns seem pretty normal from above.

We now check the qq plot -

```{r}
qqPlot(return_data$return)
```

The QQ plot shows that returns look pretty normally distributed.

We also perform the Shapiro wilk normality test.

```{r}
shapiro.test(return_data$return)
```

From the p-value = 0.4559 > 0.05, we can see that the returns are normally distributed.

This makes sense to us as in most stock price modeling, one does not model the price but rather models the return.

\newpage

### 2) Repeat question 1 from a to i on cheddar dataset from the book by fitting a model with taste as the response and the other three variables as predictors. Answer the questions posed in the first problem.

### Let us load the data and summarise the information

```{r}
# reading data
data(cheddar)
str(cheddar)
```

```{r}
#summary
summary(cheddar)
```

### Key observations -
1. taste is +vely skewed, Acetic and H2S are marginally +vely skewed  while only Lactic is negatively skewed.
2. We don't see any evidence of outliers but we will check the plots before commenting on this

### Any missing values ?

```{r}
data <-na.omit(cheddar)
str(data)
```

We did not find any missing values in the data.

### Correlation plot -

We check correlation before moving towards modeling exercise.

```{r}
M<-cor(cheddar)
head(round(M,2))
corrplot(M, method="color")
```

### Key observations -
1. We see high correlation between taste and H2S, Lactic (>0.7). We also see Acetic is correlated well with taste (>0.5).
2. We also see strong correlations between the independent variables as well (>0.6).

### Outlier/ Univariate checks -

We confirm our earlier hypothesis of no outliers by looking at some univariate and outlier checks.

```{r}
par(mfrow=c(2,5))
for (i in 1:length(cheddar)) {
        boxplot(cheddar[,i], main=names(cheddar[i]), type="l")
}

```

We see no evidence of any outliers in the univariate form.

We now proceed to modeling exercise.

\newpage

### 2a) Fit a model to explain taste in terms of the predictors. Which variables are important, can any of the variables be removed ? Please use F-tests to justify.

We use lm() function for this regression

```{r}
fit <- lm(taste~Acetic+H2S+Lactic, data=cheddar)
summary(fit)
```

We see significant result in p-value for both H2S and Lactic at 5% however we do not see Acetic acid coming as significant in the model hence this variable can be removed from the model. We get F value of 16.2 with probability <0.5 indicating the joint hypothesis of this model being better than null model.

The above model has a good R-square of 65.35%.

We now confirm this with joint hypothesis test using F-statistic. We use both anova and  linearHypothesis() for this.

### Model without Acetic 

```{r}
fit2 <- lm(taste~+H2S+Lactic, data=cheddar)
summary(fit2)
```

We now look at Anova
```{r}
anova(fit2, fit)
```

Since p-value of 0.942 is > 0.5 we conclude that the model with Acetic is not significantly better than the model without Acetic

We can see this with individual F tests for each variable using linearhypothesis() now as well.

```{r}
linearHypothesis(fit, c("Acetic=0"))
linearHypothesis(fit, c("H2S =0"))
linearHypothesis(fit, c("Lactic=0"))
```

We ran the F-tests for the hypothesis of each individual variable and in only one case is the F-score is very low (0.0054) in the 'Acetic'=0 hypothesis. Hence, this confirms from our linear regression model again that given the Pr(>F) is 0.94, we find concentration of acetic acid to not influence the taste of cheese and hence we can remove this variable from our model.

Sometimes, however it is important to run the heteroskedastic robust version of the F-test as well. We do this as follows -

```{r}
linearHypothesis(fit, c("Acetic=0"),white.adjust = "hc1")
linearHypothesis(fit, c("H2S =0"),white.adjust = "hc1")
linearHypothesis(fit, c("Lactic=0"),white.adjust = "hc1")
```

We see no change in results although the Pr(>F) for Acetic=0 hypothesis is slightly higher than before. We still only confirm that Acetic can be removed from the model.

### 2b) Check the constant variance assumption for the errors.

We do this in 3 ways - we look at scale-location plot of the regression, then perform ncvTest() and finally Breusch-pagan test for homoskedasticity.

```{r}
plot(fit,which=3)
```
\

We mostly see as they say "stars in the sky" expression in the above graph. While the line visually is not completely horizontal, we do not in particular see any pattern and it would be hard to deny that the errors are homoskedastic. However, we perform further tests to confirm our suspicion.

###  ncvTest() For Homoscedasticity

```{r}
ncvTest(fit)
```
\
We see a p-value > .05, indicating homoscedasticity.

### Breusch-Pagan Test For Homoscedasticity

```{r}
bptest(fit)
```

\
We once again see a p-value > .05, indicating homoscedasticity.

### 2c) Check the independentness of the errors assumption.

We can do this with the durbin watson statistic. The Durbin Watson examines whether the errors are autocorrelated  with themselves. The null states that they are not autocorrelated.

```{r}
durbinWatsonTest(fit)
```
\
We see that p-value > 0.05 so the errors are not autocorrelated.

### 1d) Check the normality assumption.

We again do this in 2 ways - we look at QQ plot and perform the Shapiro Wilk normality test.

The normal probability plot of residuals should approximately follow a straight line.

```{r}
plot(fit,which=2)
```
\
We see points falling mostly along reference line. We see some minor observations falling outside range but still we feel that the residuals are normal. We confirm our suspicions with a statistical test.

# Shapiro-Wilk Normality Test

```{r}
resid <- studres(fit) 
shapiro.test(resid)
```

From the p-value = 0.5444 > 0.05, we can see that the residuals are normal which satisfies the linear regression assumption.

### 1e) Is non-linearity a problem ?

The linearity assumption can be checked by inspecting the Residuals vs Fitted plot.

```{r}
plot(fit,which=1)
```
\

We see the linearity relationship mostly holds in the data indicating non-linearity isn't an issue. This might be because the acid levels are already in a log scale.

### 2f) Check for outliers, compute and plot the cook's distance.

A standard way to check for outliers is to look at residuals above a certain threshold. An example would be as follows -

```{r}
rstandard(fit)[abs(rstandard(fit)) > 1.5]
```

Here, we see points 8,12 and 15 with large residuals but note that not all of them or maybe none of them could be outliers. So we now look at the model plot of Residuals vs leverage.

```{r}
plot(fit, which=5)
```

Leverage statistic is defined as - \
$\hat{L}= \dfrac{2(p+1)}{n}$ 
where $p$ is number of predictors and $n$ is number of observations \

In the above graph we see all points fall under the dashed lines of the cook's distance (missing) which tells us there are no outliers in the data but still some influential points do exist.

We can plot the cook's distance with the below command -

```{r}
#Cook's distance
plot(fit, 4)
```

We see that apart from three points - 15, 12 and 30, everyone else's cook's distance is below 0.1

We also compute the cook's distance for each observation as follows -

```{r}
cooks.distance(fit)
```
\

### 2g) Check for influential points.

This was partially done above itself, but neverthless we can check for 
influential points through the plot itself. 

```{r}
# High leverage points
plot(fit, which=5)
```

So from this plot again, we see points
15, 12 and 30 as values of extreme nature and we see
these as influential points. 

A rule of thumb is that an observation has high influence 
if Cook's distance exceeds $\dfrac{4}{(n - p - 1)}$

```{r}
cooks.distance(fit) > 4 / length(cooks.distance(fit))
```

We see only point 15 as influential point just going by cooks distance.

We also however check for hatvalues in the data.

```{r}
hatvalues(fit) > 0.25
```

We see only few observations with hatvalues above 0.25. These are points 20 and 26.

Now, we combine the above two results of cooks distance and hatvalues which is exactly what influence.measures does for us.

```{r}
summary(influence.measures(fit))
```

The last two columns give the cooks distance and the hatvalues. We see points 6, 15, 24 and 26 as influential observations however points 6,24 and 26 have high hat value but low cooksd whereas point 15 is definitely the most influential observation in the data.

We plot the results slightly better now -

```{r}
ols_plot_cooksd_bar(fit)
ols_plot_resid_stand(fit)
ols_plot_resid_lev(fit)
```

Above we plotted the cooks d bar plot, the standarized residual plot and the rstudent vs leverage plot.

The first plot tells us that point 15 has larger cooks distance than other points
The second plot tells us that points 15 again has larger residuals
And the third plot tells us that if we use a leverage threshold of 0.267 we do not get any outlier but we see 15 as highly influential given the large residual.

\newpage

### 3) The problem is to discover relation between US new house construction starts data (HOUST) and macro economic indicators : GDP, CPI, and POP. 

### a) Combine all data into an R dataframe object and construct dummy or factor variable for 4 quarters. First model is HOUST~GDP+ CPI + quarter

We first load all the files and look at their structures-

### Loading files

```{r}
# in the cpi file, the data starts from row 55 onwards so we skip the first 54 rows
cpi <- read_excel('/Users/mac/Downloads/final-2020-canvas/datasets/House/CPI.xls',skip=54)
# in the gdp file, the data starts from row 19 onwards so we skip the first 18 rows
gdp <- read_excel('/Users/mac/Downloads/final-2020-canvas/datasets/House/GDP.xls',skip=18)
# in the pop file, the data starts from row 11 onwards so we skip the first 10 rows
pop <- read_excel('/Users/mac/Downloads/final-2020-canvas/datasets/House/POP.xls',skip=10)
# in the houst file, the data starts from row 11 onwards so we skip the first 10 rows
houst <- read_excel('/Users/mac/Downloads/final-2020-canvas/datasets/House/HOUST.xls',skip=10)
```

```{r}
str(cpi)
str(gdp)
str(pop)
str(houst)
```

### Key observations -

1. We see that cpi has 161 observations from 1976-01-01 till 2016-01-01 \
2. We see that gdp has 163 observations from 1976-01-01 till 2016-07-01 \
3. We see that pop has 160 observations from 1976-01-01 till 2015-10-01 \
4. We see that houst has 161 observations from 1975-10-01 till 2015-10-01. \

We use a nested merge to merge the 4 files. Do note, we do not include some observations that only have a GDP value or only a CPI value. We take the data from 1976 1st quarter till 2015 last quarter this way.

### Merging the 4 datasets 

We merge the four datasets and convert the DATE variable from POSIXct to date format.

```{r}
data  <- merge(merge(merge(houst,cpi, by.x='observation_date', by.y='DATE')
               ,gdp, by.x='observation_date', by.y='DATE')
               ,pop, by.x='observation_date', by.y='observation_date')
colnames(data)[1] <- "DATE"
colnames(data)[2] <- "HOUST"
colnames(data)[3] <- "CPI"
colnames(data)[4] <- "GDP"
colnames(data)[5] <- "POP"
# we also convert the date variable from posixct to date format
data$DATE <- as.Date(data$DATE)
str(data)
```

### Constructing the dummy variable for each quarter as 1-4

```{r}
data$quarter = as.factor(substr(as.yearqtr(data$DATE),7,8))
str(data)
```

We can also create individual factor variables from below and add them as binary variables to the data

```{r}
quarter_data <- data.table::dcast(data, DATE ~ paste0("Q", lubridate::quarter(data$DATE)), length, 
                  value.var = "DATE")
houst_data <- merge(data,quarter_data,by=c("DATE"))
houst_data$Q1 <- as.factor(houst_data$Q1)
houst_data$Q2 <- as.factor(houst_data$Q2)
houst_data$Q3 <- as.factor(houst_data$Q3)
houst_data$Q4 <- as.factor(houst_data$Q4)
str(houst_data)
```

We now fit the first model as in question

```{r}
fit <- lm(HOUST~GDP+CPI+quarter, data=houst_data)
summary(fit)
```

We see a model with R-sq of 17.8%. We notice how the intercept term is significant. The GDP is very close as well (0.06 p-value) however the significant terms in the model are quarter 2 and quarter 3. CPI is not significant here.

It doesn't matter if we use the binary variables as well. We get the same R-square and model. Do note we don't put all 4 quarter variables due to collinearity issues but choose only Q2-Q4 here.

```{r}
fit <- lm(HOUST~GDP+CPI+Q2+Q3+Q4, data=houst_data)
summary(fit)
```

### 3b) Do you think the data needs some cleaning. If so, clean the data.

We start by checking the summary of data.

```{r}
summary(houst_data)
```

### Key observations -
1. We see that CPI and GDP have some negative values. In theory this is possible during negative inflation and recession in the economy can make the GDP growth negative.

Let's check their scale.

```{r}
which(houst_data$CPI < 0)
which(houst_data$GDP < 0)
```

We see 10 observations with negative CPI (~6.25%) \
We see 6 observations with negative GDP (~3.75%) \
We see 2 observations with both -ve CPI and -ve GDP \

Let's delete the data and refit the model.

```{r}
houst_data_clean <- houst_data[houst_data$CPI >= 0, ]
houst_data_clean <- houst_data_clean[houst_data_clean$GDP >= 0, ]
str(houst_data_clean)
```

We removed 14 observations.

We now look at the model plots.

```{r}
plot(fit)
plot(fit,which=4)
```

Although we see some influential observations like 132, 138 and 142, we do not see any outlier significant enough to be removed from the data.


### 3c) Is there a seasonal effect you observe in the data ? Show necessary steps and explanation. 

We convert our variables to a time series in R first and then use decompose() function to decompose the time series into random, trend, seasonal and observed components. We can do this only for the Houst variable but we do it for all explanatory variables as well.

```{r}
houst_ts <- ts(houst_data$HOUST,frequency = 4)
houst_components <- decompose(houst_ts)
plot(houst_components)

cpi_ts <- ts(houst_data$CPI,frequency = 4)
cpi_components <- decompose(cpi_ts)
plot(cpi_components)

gdp_ts <- ts(houst_data$GDP,frequency = 4)
gdp_components <- decompose(gdp_ts)
plot(gdp_components)

pop_ts <- ts(houst_data$POP,frequency = 4)
pop_components <- decompose(pop_ts)
plot(pop_components)

```


We analyze the houst graph above and see major spikes at period 2 and minor spike in period 3 with a dip followed in period 4. Clearly, this indicates spike at Q2 and Q3 for new privately owned homes.

```{r}
plot(houst_ts, main="Trend and Seasonal Quarterly Data", ylab="New privately owned homes")
cma_houst <- cmav(houst_ts, outplot=1)
plot(cpi_ts, main="Trend and Seasonal Quarterly Data", ylab="CPI")
cma_cpi <- cmav(cpi_ts, outplot=1)
plot(gdp_ts, main="Trend and Seasonal Quarterly Data", ylab="GDP")
cma_gdp <- cmav(gdp_ts, outplot=1)
plot(pop_ts, main="Trend and Seasonal Quarterly Data", ylab="POP")
cma_pop <- cmav(pop_ts, outplot=1)
```
### We can test for seasonality visually from the plot as well

We now use seasplot(). The function shows both trend and seasonality test in the data.

```{r}
seasplot(houst_ts)
seasplot(cpi_ts)
seasplot(gdp_ts)
seasplot(pop_ts)
```

### Key observations -
1. We find evidence for trend and seasonality in new privately owned homes (HOUST). \
2. We find no evidence for trend or seasonality in CPI. \
3. We find evidence for trend however none for seasonality in GDP \
1. We find evidence for trend and seasonality in POP. \

We also check for other seasonality plots for one of the above (Houst).

```{r}
seasplot(houst_ts, outplot =2)
seasplot(houst_ts, outplot =3)
seasplot(houst_ts, outplot =4)
```

We plot the seasonal box plot, sub series, and distribution above. Notice how we get outlier point in Q2, Q3 and Q4 since we haven't used the clean data for this plot. We also see the -ve skew in Q2 due to a sharp dip as seen in the subseries.

Another way to test for seasonality is the ACF plot.

```{r}
acf(houst_ts, lag.max = 36)
```

Generally, a time series with a seasonality component tend to start at a large value and decrease over time. We see how the above series for Houst follows this.

Finally, we now perform statistical tests to detect seasonality. Both Student t-test and Wilcoxon Signed Rank test are considered good tests for detection. We will however use something very recent - WO-test, i.e. the overall seasonality test developed in Webel and Ollech (2019).

```{r}
summary(wo(houst_ts))
summary(wo(gdp_ts))
summary(wo(cpi_ts))
summary(wo(pop_ts))
```

This test confirms our earlier results that there is clear seasonality in the data of New privately owned homes and population but not for CPI and GDP.

### 3d) Do a pair-wise comparison for different quarters. Which quarter do you think is the best to buy a house ? Show necessary steps and explanation. 

We check anova to see if there is a significant difference between number of construction starts between various quarters of the year.

```{r}
anova_mod <- aov(formula = HOUST ~ quarter,
                   data = houst_data)
summary(anova_mod)
```

Since p-value is < 0.5 we conclude that there is a significant difference between number of construction starts between any two quarters of the year.

WE also show that even with other variables, quarter is signifcant using ANOVA.

```{r}
anova_res <- aov(HOUST~GDP+CPI+quarter, data=houst_data)
summary(anova_res)
```

Now we compute pair-wise comparisons for different quarters - we can do this using TukeyHSD or pairwise.t.test.

```{r}
TukeyHSD(anova_mod, conf.level = 0.95)
```

```{r}
pairwise.t.test(houst_data$HOUST, houst_data$quarter, p.adj = "none")
```

We see interestingly that Q1-Q2, Q1-Q3, Q2-Q4 & Q3-Q4 are significant in pair-wise comparisons. In fact, the only non-significant pairs are Q1-Q4 and Q2-Q3.

```{r}
fit <- lm(HOUST~GDP+CPI+quarter, data=houst_data)
summary(fit)
```

Recall our model results now. We saw the coefficient of Q2 being the largest followed by Q3 and then Q4. It seems that more people buy homes in Q2 and Q3 than in Q1 and Q4. It makes sense as well since most people buy properties in Spring or Summer and not the winter months. Given the boxplot spread as well since most construction starts happen in Q2, prices will be lower and we can recommend this for a person not looking for ready to move in investment.

Although it is hard to say what is the best time to buy a month based on this data given we don't have any indication of demand. For instance, people may choose to buy during months they feel the prices will be lower, or demand will be lesser however this data only provides us with an indication of the economy strength and the number of privately owned homes in each quarter. A customer knowing that demand is high in Q2 may choose to buy in Q4 when demand is low and prices are lower compared to Spring-Summer months.

However, we take the former answer for now and recommend that the best time to buy a house given the strength of the economy would be in Q2 (the quarter with the highest coefficient in indicating when the number of homes goes up).

### 3e) Add population to the first model. Do steps b and c again.

We add population to the model now.

```{r}
fit <- lm(HOUST~GDP+CPI+POP+quarter, data=houst_data)
summary(fit)
```

We see R-square marginally improves to 17.99% however we do not see population as a significant predictor of HOUST.

### Steps for b - data cleaning in pop ?
Recall we did not see any data cleaning requirements in population variable earlier itself. We can however check the distribution by a boxplot for population and see if there are any outliers.

```{r}
ggplot(houst_data, aes(x = factor(0), y = POP)) + geom_boxplot() + xlab("") +
  scale_x_discrete(breaks = NULL)
```

We do not observe any outliers in the variable.

Now let's check the plots of the model.

```{r}
plot(fit)
plot(fit,which=4)
```

We again see no outliers hence no need for cleaning. We also already noted the variable isn't significant so we do not need any further cleaning.

### Steps for c - Seasonality in pop.

Recall we have already shown earlier the seasonality trend in population when we were solving 3c). We in fact detected Seasonality for all explanatory variables there. Here's a brief summary of what we did for poopulation.

```{r}
pop_ts <- ts(houst_data$POP,frequency = 4)
pop_components <- decompose(pop_ts)
plot(pop_components)
plot(pop_ts, main="Trend and Seasonal Quarterly Data", ylab="POP")
cma_pop <- cmav(pop_ts, outplot=1)
seasplot(pop_ts)
seasplot(pop_ts, outplot =2)
acf(pop_ts, lag.max = 36)
summary(wo(pop_ts))
```

We detected seasonality in population from the above. In fact, this is very similar to the HOUST variables as we see spikes in Q2-Q3 and dip in Q1 and Q4.

\newpage

### 4. Read the train.csv and test.csv files in R which contain training and test information on 10,000 customers. Aim is to predict wihch customers will default on their credit card debt. These datasets contain the following information/ variables - default, student, balance, income.

### Let us load the data and summarise the information

```{r}
# reading data
train_data <- read.csv('/Users/mac/Downloads/final-2020-canvas/datasets/train-default.csv')
test_data <- read.csv('/Users/mac/Downloads/final-2020-canvas/datasets/test-default.csv')
str(train_data)
str(test_data)
```

```{r}
#summary
summary(train_data)
summary(test_data)
```

### Key observations -
1. Defaulters in the training data are quite low (3.22%)
2. Students are about 29.8% in the training data
3. We see balace is marginally +vely skewed while income is marginally -vely skewed
4. We don't see any evidence of outliers but we will check the plots before commenting on this

### Any missing values ?

```{r}
training_data <-na.omit(train_data)
str(training_data)
testing_data <-na.omit(test_data)
str(testing_data)
```

We did not find any missing values in the data.

### Let us convert the No-Yes in default and student columns to 0-1.

```{r}
train_data <- train_data %>%
      mutate(default = ifelse(default == "No",0,1)) %>%
      mutate(student = ifelse(student == "No",0,1))
test_data <- test_data %>%
      mutate(default = ifelse(default == "No",0,1)) %>%
      mutate(student = ifelse(student == "No",0,1))
str(train_data)
```

### Correlation plot -

We check correlation before moving towards modeling exercise.

```{r}
M<-cor(train_data)
head(round(M,2))
corrplot(M, method="color")
```

### Key observations -
1. We see default is +vely correlated with balance although the relation isn't very strong (0.35) \
2. We also see default being marginally -vely correlated to income as individuals with higher income may have lower default. \
3. We see strong negative correlation (-0.75) between student and income which is understandable. \
4. We also see weak positive correlation between student and balance and weak negative relation between income and balance. \
5. A theme that comes out here is that students with low to no income may have higher likelihood of defaulting. On the contrary, we note that students credit card debt if taken care of by parents would depend on their income. We can construct such hypothesis before proceeding to modeling exercise.

### Outlier/ Univariate checks -

We create a quick boxplot of all variables.

```{r}
par(mfrow=c(2,5))
for (i in 1:length(train_data)) {
        boxplot(train_data[,i], main=names(train_data[i]), type="l")
}

```

We see some evidence of outliers in balance. Let us look closely to understand if this is data issue or actual high balance of customers.

### Plotting balance

```{r}
ggplot(train_data, aes(x=balance)) + 
  geom_density(color="darkblue", fill="lightblue")
```

We see how the balance plot tapers down above a balance of 2000.There is not much reason to think that these balances are outliers at the moment.

Armed with the EDA above, We now proceed to modeling exercise.

### 4a) Fit logistic regression with default as the response and other variables balance and income as the predictor. Make sure the variables in your model are significant. Perform regression diagnostics. Display any relevant plots.

We convert default and student back to factor for modeling.

```{r}
train_data$default <- factor(train_data$default)
train_data$student <- factor(train_data$student)

test_data$default <- factor(test_data$default)
test_data$student <- factor(test_data$student)

str(train_data)
```

### Let's check VIF

```{r}
mod <-  lm(as.numeric(default) ~  balance + income, data = train_data) 
summary(mod)
vif(mod)
```

We see both variables are significant in linear regression and VIF is small ~ 1.019. This is good news.

### Two-way contingency table of categorical outcome and predictors 
### Since we want to make sure there are not 0 cells

```{r}
xtabs(~train_data$default + train_data$student, data = train_data)
xtabs(~test_data$default + test_data$student, data = test_data)
```

Great news. Unlike Assignment 5, we see here there is no issue of zero contingency and a normal logistic regression should be fine.

### Fitting logistic model

```{r}
set.seed(123)
mylogit <- glm(default ~ balance + income , data = train_data, family = binomial(link="logit"))
summary(mylogit)
```

We see that both balance and income are significant predictors of defaulting on the credit card. Our model has an AIC value of 907.62.


### Let's predict the outcome for this model
```{r}
predicted <- predict(mylogit, test_data, type="response")
```


### Deciding on optimal cutoff
```{r}
optCutOff <- optimalCutoff(test_data$default, predicted)[1] 
optCutOff
```

\
This tells us that we can use this prob cutoff to classify an 
observation as 0 or 1. If the prob-value is below this threshold we
can classify it as non-defaulter else the customer is a defaulter.

### Mis-classification error
```{r}
misClassError(test_data$default, predicted, threshold = optCutOff)
```

\
We note a misclassification error on test set of 2.7%

### ROC curve
```{r}
plotROC(test_data$default, predicted)
```

We see an AUC of 0.91 which is quite good.

### Using optimal cutoff to determine accuracy measures with positive class as 1
```{r}
threshold=optCutOff
predicted_values<-ifelse(predict(mylogit, test_data,
                            type="response")>threshold,1,0)
actual_values<-test_data$default
confusionMatrix(data = as.factor(predicted_values), 
                reference = as.factor(actual_values), 
                positive='1',mode = "prec_recall")
```

We see an overall accuracy of ~97.2% with precision of 0.79, recall of 0.28 and an F1-score of 0.41

### Manipulating cutoff to increase recall
```{r}
threshold=0.25
predicted_values<-ifelse(predict(mylogit, test_data,
                            type="response")>threshold,1,0)
actual_values<-test_data$default
confusionMatrix(data = as.factor(predicted_values), 
                reference = as.factor(actual_values), 
                positive='1',mode = "prec_recall")
```

We see an overall accuracy of ~96.2% with precision of 0.46, recall of 0.47 and an F1-score of 0.46

### 4b) Why is your model a good/ reasonable model ? Check AIC and pseudo R square values.

Our model is a decent model we can say as we recall the statistics.

AIC - 908.61
Misclassification rate - 2.7%
AUC - 0.91
Accuracy - 97.2%
Precision - 79%
Recall - 28 %
F1-Score - 0.41

Overall the model statistics are fine but we see a very low recall. Hence, we changed the probability cutoff to 0.25 instead of 0.51 to increase recall of the model. In this problem, we cannot afford such a low recall since we want to be able to identify the defaulter with more certainty.

### Pseudo- Rsquare for Logistic regression

```{r}
nullmod <- glm(train_data$default~1, family="binomial")
r_sq <- 1-logLik(mylogit)/logLik(nullmod)
r_sq
```

```{r}
summary_glm <- summary(mylogit)
list( summary_glm$coefficient, 
      round( 1 - ( summary_glm$deviance / summary_glm$null.deviance ), 2 ) )
```

We get a pseduo R-square of 0.48.

### 4c) Give interpretation of regression coefficients.

We can write our model form as -

$log(\dfrac{p}{1-p}) = -1.173178e+01 +  5.787483e-03 * balance + 1.718741e-05 * income$

```{r}
coeffs <- round(coefficients(mylogit), digits = 4)

exp(coeffs[[1]]); exp(coeffs[[2]]); exp(coeffs[[3]]) 

```

We now interpret the coefficients as -

Keeping income constant, for a unit increase in balance, the odds of defaulting on a credit card increase by 1.0058 or by 0.58%.

Similarly, Keeping balance constant, for a unit increase in income, the odds of defaulting on a credit card increase by 1.

We can interpet the intercept term as the odds of defaulting on a credit when balance and income are both 0.

### 4d) Form confusion matrix over test data. What % of the time are your predictions correct ?

We did this in 4a) itself however we show here again.

```{r}
threshold=optCutOff
predicted_values<-ifelse(predict(mylogit, test_data,
                            type="response")>threshold,1,0)
actual_values<-test_data$default
confusionMatrix(data = as.factor(predicted_values), 
                reference = as.factor(actual_values), 
                positive='1',mode = "prec_recall")
```
```{r}
threshold=0.25
predicted_values<-ifelse(predict(mylogit, test_data,
                            type="response")>threshold,1,0)
actual_values<-test_data$default
confusionMatrix(data = as.factor(predicted_values), 
                reference = as.factor(actual_values), 
                positive='1',mode = "prec_recall")
```           

Our model accuracy is 97.29% so 97.29% times our predictions are correct however we increased our recall in the second iteration changing the prob cutoff from 0.51 to 0.25 in order to predict defaulters better. For that model, we are correct 96.2% times.

### 4e) In your model, what is the estimated probability of default for a student with a credit card balance of of $2000 and income of $40000. What is the prob. of default for a non-student with the same credit card balance and income ?

Since, we did not include student to our model, we don't expect the probabilities of default to be different for the same levels of income and balance. Neverthless, we compute

```{r}
x0 = data.frame(student=1,balance = 2000, income = 40000, stringsAsFactors = FALSE)
predict(mylogit, x0, type="response")
x0 = data.frame(student=0,balance = 2000, income = 40000, stringsAsFactors = FALSE)
predict(mylogit, x0, type="response")
```

As, we can see the prob of default is 0.629 irrespective of whether the customer is a student or not.

### 4f) Are the variables balance and student correlated ? If yes, why ? If no, explain.

We first plot them together -

```{r}
ggplot(train_data,
       aes(x = factor(student),
           y = balance,
           group = student,
           fill = factor(student))) +
  geom_boxplot() +
  theme_light() +
  theme(legend.position = "none") +
  labs(title = "Boxplot of Balance for Students and Non-Students",
       x = "Student Status",
       y = "Balance")
```

It seems the means of balance for student vs non-student are not that different.

And now we check the correlation between the variables -

```{r}
M<-cor(train_data$balance,as.numeric(train_data$student))
head(round(M,2))
```

Recall earlier we computed this correlation in 4) as well. The variables are very weakly +vely correlated (0.2). We do not see why being a student should indicate a higher balance on the credit card though.

### 4g) Now let's add the binary variable student to the model. 

```{r}
set.seed(123)
mylogit <- glm(default ~ balance + income + student + 0 ,
              data = train_data, family = binomial(link="logit"))
summary(mylogit)
```

We see that the variable income is no longer significant in this model.

### 4h) Does the data say that it is more likely for a student to default compared to non-student for different values of income level. Please comment.

We compute -

```{r}
coeffs <- round(x = coefficients(mylogit),
                digits = 4)
coeffs
```

The coefficient of student1 is -11.7165 while the coefficient of student0 is -10.9070 hence the data suggests that the odds of defaulting for a student are lower compared to non-student. Hence, for different levels of income, the result would remain the same.

\newpage

### 5. These days there is a lot of discussion on how heathcare system should look like in the US. For a scientific discussion, one needs to have a model of demand in the healthcare system. In this question, we work on dvisit which is about modeling the demand for doctor visits in terms of explanatory variables such as age, income, existence of health insurance, others.

### Let us load the data and summarise the information

```{r}
# reading data
data(dvisits)
str(dvisits)
```

This data is from the Australian health survey of 1977-78 where 5190 single adults where yound and old have been oversampled. The definitions are as follows - \

1. sex : 1 if female, 0 if male \
2. age : Age in years divided by 100 measured as mid point of 10 age groups from 15-19 to 65-69 with 70+ coded as 72 \
3. agesq : age squared \
3. income : Annual income in australian dollars divided by 1000. 14000+ income is coded as 15000 \
4. levyplus : 1 if covered by private health insurance fund for private patient in public hospital with doctor of choice, 0 otherwise \
5. freepoor : 1 if covered by govt. because low income, recent, immigrant, unemployed, 0 otherwise \
6. freerepa : 1 if covered free by govt. because of old age or disability pension, or because invalid veteran or family of deceased veteran, 0 otherwise \
7. illness : Number of illnesses in past two weeks capped at 5 for 5+ \
8. actdays : Number of days of reduced activity in past two weeks due to injury or illness \
9. hscore : General health questionnaire score using Goldberg's method. High score indicates bad health \
10. chcond1 : 1 if chronic condition but not limited in activity, 0 otherwise \
11. chcond2 : 1 if chronic condition and limited in activity, 0 otherwise \
12. doctorco : Number of consulations with a doc in last two weeks \
13. nondocco : Number of consulations with non-doc in last two weeks \
14. hospadmi : Number of admissions to a hospital in past 12 months \
15. hospdays : Number of nights in a hospital during most recent admission. No admissions in past 12 months is coded as 0 and 80+ is coded as 80 \
16. medicine : Total number of prescribed and nonprescribed medications used in past 2 days \
17. prescrib : Total number of prescribed medications in past 2 days \
18. nonpresc : Total number of non-prescribed medications in past 2 days \

We now analyse the data before developing the model.

```{r}
#summary
summary(dvisits)
```

### Key observations -
1. We see age is +vely skewed but understandable due to oversampled dataset. \
2. Given most of the capping logic already present, we may not see any outliers. \
3. Our dependent variable seems to have a lot of 0 days given the 3rd quartile is also 0. On average however 1.3 nights are spent in hospital. \
4. Medicine seems to be the sum of prescrib and nonpresc. \

### Any missing values ?

```{r}
data <-na.omit(dvisits)
str(data)
```

We did not find any missing values in the data.

### Correlation plot -

We check correlation before moving towards modeling exercise.

```{r}
M<-cor(dvisits)
head(round(M,2))
corrplot(M, method="color")
```

### Key observations -
1. We see hospdays is correlated with hospadmi (+0.5) which seems to indicate that number of admissions in past 12 months is correlated with number of nights in hospital in the last admission. This maybe case of causation but we will explore more in detail later. \
2. We also see a negative correlation between hospdays and each of levyplus, income and freepoor. \
3. Age seems to be weakly positively correlated with medicine especially prescribed ones but negatievly with non prescribed ones. \
4. Income is mostly negatively correlated with most other explanatory variables as well as hospdays. \
5. Levyplus and freerepa are negatively correlated (-0.46). This makes sense as the variables signify private care vs public care.
6. Doctorco and illness are correlated +vely as well.
7. prescrib and medicine are most +vely strongly correlated which is expected.

### Outlier/ Univariate checks -

We confirm our earlier hypothesis of no outliers by looking at some univariate and outlier checks.

```{r}
par(mfrow=c(2,5))
for (i in 1:length(dvisits)) {
        boxplot(dvisits[,i], main=names(dvisits[i]), type="l")
}

```

We see actdays, hscore, doctorco, nondocco, hospadmi, hospadys and medicine (both prescrib and nonpresc) having some outliers.

### Let's check how many zeros are in dataset
```{r}
colSums(dvisits==0)
# Let's check their proportion to dataset as well 
round(colSums(dvisits==0)/nrow(dvisits)*100,2)
```

We see some variables like freepoor, actdays, chcond2, nondocco, hospadmi and hospdays with > 85% of values as 0.

### Plotting the univariate plots -

We now plot the histogram/ density functions of some numerical variables.

### Age

```{r}
hist(dvisits$age, main="Histogram of Age", 
     xlab="Age ", 
     border="white", 
     col="dark blue",
     las=1)
```

We confirm visually that age for young and old seem oversampled as mentioned.

### Age sq

```{r}
hist(dvisits$agesq, main="Histogram of Age Sq", 
     xlab="Agesq ", 
     border="white", 
     col="dark blue",
     las=1)
```


### Illness

```{r}
hist(dvisits$illness, main="Histogram of Illness", 
     xlab="Illness ", 
     border="white", 
     col="dark blue",
     las=1)

```

Majority of the illnesses are either 0 or 1 in the past two weeks.

### Actdays

```{r}
ggplot(dvisits, aes(x=actdays)) + 
  geom_density(color="darkblue", fill="red") +
  ggtitle("Actdays distribution")
```

Majority of the days of reduced acitvity are concentrated around 0 with small spike at 15.


### Hospdays

```{r}
hist(dvisits$hospdays, main="Histogram of Hospdays", 
     xlab="Hospdays", 
     border="white", 
     col="dark blue",
     las=1)

ggplot(dvisits, aes(x=hospdays)) + 
  geom_density(color="darkblue", fill="red") +
  ggtitle("Hospdays distribution")

```

Most of the number of nights in hospital during recent admission are 0 however we see some outliers above 40 nights. We also note the incredibly skewed distribution and infer that a log transformation may be apt to model this response variable.

### Hospadmi

```{r}
hist(dvisits$hospadmi, main="Histogram of Hospadmi", 
     xlab="Hospadmi ", 
     border="white", 
     col="dark blue",
     las=1)
```

Majority of the admissions to hospital were 0, and next 1 admission in the past 12 months.

### 5a) Using dvisits dataset fit a model with hospdays as response and other variables as potential predictors. Perform regression diagnostics on the model. Display any relevant plots.

We now perform linear regression using all variables in first iteration.

### First iteration

We create the train and test split first 85% - 15% and fit the first model.

```{r}
set.seed(123)
dvisits_train <- dvisits %>% sample_frac(.85)
dvisits_test  <- anti_join(dvisits, dvisits_train)

fit <- lm(hospdays~age+agesq+income+levyplus+
            freepoor+freerepa+illness+actdays+
            hscore+chcond1+chcond2+doctorco+
            nondocco+hospadmi+medicine+
            prescrib+nonpresc, data=dvisits_train)
summary(fit)
vif(fit)
```

We see an R-square of 27.5% however we now remove the non-significant variables and the highly correlated ones.

### Second iteration

```{r}
fit <- lm(hospdays~agesq+freerepa+actdays+
            chcond2+nondocco+hospadmi, data=dvisits_train)
summary(fit)
vif(fit)
AIC(fit)
```

We now see an R-square of 26.3% and our VIF is good (maximum VIF of 1.6 for freerepa) and we have all our variables as significant. Our AIC however is quite large at 27,369.21

We can see that all of our variables have a positive sign indicating that as age, free coverage by govt., days of reduced activity, chronic condition and limited in activity, consultations with non doctor staff and number of admissions (our significant explanatory variables) go up/ increase, consequently the number of nights in most recent admission (our dependent variables) go up/ increase as well.

### Third iteration

We now transform our response variables using four methods -

1. log transformation
2. GLM with log link
3. the boxcox transformation by first estimating lambda
4. transforming predictors the brute way

### 1. Log transform of the dependent variable

```{r}
fit <- lm(log(hospdays+1)~agesq+freerepa+actdays+
            chcond2+nondocco+hospadmi, data=dvisits_train)
summary(fit)
vif(fit)
AIC(fit)
```

We see a significant jump in R-square in this iteration to 63.7% which is great as our variables are still all significant. Our AIC comes down to 5,358 in this iteration.

### 2. GLM with log link

```{r}
# add a small value to hospdays
glm.mod <- glm(hospdays+0.0001~agesq+freerepa+actdays+
            chcond2+nondocco+hospadmi, 
            data=dvisits_train, family=gaussian(link="log"))
summary(glm.mod)
vif(glm.mod)
AIC(glm.mod)
```

This isn't a great model as actdays isnt signifcant now and AIC has gone back up to 28,119.

### 3. the boxcox transformation by first estimating lambda

```{r}
boxCox(fit, family="yjPower", plotit = TRUE)
```

We see -2 as lambda where log likelihood is maximized.

```{r}
dvisits_train$depvar.transformed <- yjPower(dvisits_train$hospdays, -2)
```

```{r}
fit <- lm(depvar.transformed~agesq+freerepa+actdays+
            chcond2+nondocco+hospadmi, data=dvisits_train)
summary(fit)
vif(fit)
AIC(fit)
```

This model doesn't work as most of our predictors are now not signifcant.

### 4. Transforming the predictors in a brute force way 

This method is used in applications in the real world to understand if a transformation of a predictor variable works better on the response. Let's see how this works.

We use multiple transformations of square, cube, log, inverse, inverse squared. We do not use sqrt here but for exposition, this method will work well.

```{r}
# we first add an insignificant value to variables which have a 0 in them
dvisits$actdays <- dvisits$actdays+0.00001
dvisits$nondocco <- dvisits$nondocco+0.00001
dvisits$hospadmi <- dvisits$hospadmi+0.00001

fit <- lm(log(hospdays+1) ~ agesq+I(agesq^2) +
            I(agesq^3) + I(agesq^-1) + I(agesq ^-2)
          + I(log(agesq+1))+
            freerepa+
actdays+I(actdays^2) + I(actdays^3) + I(actdays^-1)
+ I(actdays ^-2) + I(log(actdays+1))+
nondocco+I(nondocco^2) + I(nondocco^3) +
  I(nondocco^-1) + I(nondocco ^-2) + I(log(nondocco+1))+
hospadmi+I(hospadmi^2) + I(hospadmi^3) +
  I(hospadmi^-1) + I(log(hospadmi+1))+
chcond2
  , data=dvisits)
summary(fit)
vif(fit)
```

Given we fitted all transformations its time to use only the significant ones but with good coefficient from the above result. We do this below.

```{r}
fit <- lm(log(hospdays+1) ~  I(log(agesq+1))+
freerepa+
I(actdays ^-2) + actdays +
nondocco +
I(hospadmi^3) + hospadmi +
chcond2
  , data=dvisits)
summary(fit)
vif(fit)
AIC(fit)
```

We are further able to improve our model R-square to 74.6% by using the above transformations and our AIC of the new model is 4403 ( a significant improvement). This is great as our new model equation includes the log transformation of agesq, inverse squared of actdays, cube of hospadmi in addition to previous significant variables. 

We will stop here for now and solve the questions asked before proceeding to more complex models which will be done at the end as continued third iteration onwards.

We note our two models as -

1) base model

```{r}
base_model <- lm(hospdays~agesq+freerepa+actdays+
            chcond2+nondocco+hospadmi, data=dvisits_train)
summary(base_model)
vif(base_model)
AIC(base_model)

```
2) transformed model

```{r}
transformed_model <- lm(log(hospdays+1) ~  I(log(agesq+1))+
freerepa+
I(actdays ^-2) + actdays +
nondocco +
I(hospadmi^3) + hospadmi +
chcond2
  , data=dvisits)
summary(transformed_model)
vif(transformed_model)
AIC(transformed_model)
```

### Note : We answer the below questions now from the point of view of the base model

### 5b) Why is your model a good/ reasonable model ? Check the constant variance assumption for errors.

We explore this question first for our base model -

We see an R-square of 26.5% already and we saw our VIFs are good and indicate absence of multicollinearity between the variables. So we can now plot the model.

### Let's plot the results
```{r}
plot(base_model)
```

### Checking constant variance assumption

We can see visually from the scale-location plot that the residuals increase with fitted values indicating heteroskedasticity.

###  ncvTest() For Homoscedasticity

```{r}
ncvTest(base_model)
```
\
We see a p-value < .05, indicating heteroscedasticity.

### Breusch-Pagan Test For Homoscedasticity

```{r}
bptest(base_model)
```

\
We once again see a p-value < .05, indicating heteroscedasticity.

### Let's predict the model and compute accuracies

```{r}
#Create the evaluation metrics function

eval_metrics = function(model, df, predictions, target){
  resids = df[,target] - predictions
  resids2 = resids**2
  N = length(predictions)
  r2 = as.character(round(summary(model)$r.squared, 2))
  adj_r2 = as.character(round(summary(model)$adj.r.squared, 2))
  print(adj_r2) #Adjusted R-squared
  print(as.character(round(sqrt(sum(resids2)/N), 2))) #RMSE
}
```

```{r}

#Predicting and evaluating the model on train data
predictions = predict(base_model, newdata = dvisits_train)
eval_metrics(base_model, dvisits_train, predictions, target = 'hospdays')

#Predicting and evaluating the model on test data
pred <- predict(base_model,dvisits_test)
rmse <- sqrt(sum((pred - dvisits_test$hospdays)^2)/length(dvisits_test$hospdays))
mape <- mean(abs(dvisits_test$hospdays - pred)/(dvisits_test$hospdays+0.01))
med_ape <- median(abs(dvisits_test$hospdays - pred)/(dvisits_test$hospdays+0.01))
c(R2=summary(base_model)$r.squared,RMSE = rmse, MAPE = mape, MED_APE=med_ape)
```

We see that our base model gives R-square of 26.3% 
and a MAPE of 5289% with median mape of 2432% and 
an RMSE of 4.9. Our AIC was very large at 27369.21
We violate the homoskedasticity assumption as well.
Clearly, we do not feel this is not a good model.

### 5c) Check the normality assumption

We again do this in 2 ways - we look at QQ plot and perform the Shapiro Wilk normality test.

The normal probability plot of residuals should approximately follow a straight line.

```{r}
plot(base_model,which=2)
```
\
We see points drastically falling away from the normal line.

### Shapiro-Wilk Normality Test

```{r}
resid <- studres(base_model) 
shapiro.test(resid)
```

From the p-value  < 2.2e-16 < 0.05, we can see that the residuals are not normally distributed.

### 5d) Are the errors correlated ?

We can do this with the durbin watson statistic. The Durbin Watson examines whether the errors are autocorrelated  with themselves. The null states that they are not autocorrelated.

```{r}
durbinWatsonTest(base_model)
```
\
We see that p-value = 0.924 > 0.05 so the errors are not autocorrelated.

### 5e) Check for leverage points, outliers and influential points.

A standard way to check for outliers is to look at residuals above a certain threshold. An example would be as follows -

```{r}
rstandard(base_model)[abs(rstandard(base_model)) > 13]
```

Here, we see points 12, 304, 852, 2050, 2684, 140, 1490 and 3080 with large residuals but note that not all of them or maybe none of them could be outliers. So we now look at the model plot of Residuals vs leverage.

```{r}
plot(base_model, which=5)
```

Leverage statistic is defined as - \
$\hat{L}= \dfrac{2(p+1)}{n}$ 
where $p$ is number of predictors and $n$ is number of observations \

In the above graph we see all points fall under the dashed lines of the cook's distance apart from point 948 indicating we clearly have one outlier for sure this time. We also see some points of high leverage and slightly higher residuals on the right corner as well. These however could be influential points.

We can plot the cook's distance with the below command -

```{r}
#Cook's distance
plot(base_model, 4)
```

We see that apart from three points - 948, 965 & 1180, everyone else's cook's distance is below 0.2

We claim the other two points as influential points.

A rule of thumb is that an observation has high influence 
if Cook's distance exceeds $\dfrac{4}{(n - p - 1)}$

Note that given the size of the data, we do not compute cooks distance and hatvalues like we did earlier but of course this can be done as well.

We plot the results slightly better now -

```{r}
ols_plot_cooksd_bar(base_model)
ols_plot_resid_stand(base_model)
ols_plot_resid_lev(base_model)
```


### 5f) Check the structure of the relationship between the predictors and the response.

This is interesting question, because while creating our transformed model we were able to partially answer this question. Refer to 5a) to see our transformed model.

Let us first check the linearity assumption in our base model.

```{r}
plot(base_model,which=1)
```

\
In this plot, we clearly see a pattern for residuals
We see them decreasing from 0 to 3 and above 10 (fitted values) and increasing
below 10 and above 3 (fitted values). This indicates we don't have linear
relationship between our dependent and independent variables.

Let us also check the linearity assumption in our transformed model.

```{r}
plot(transformed_model,which=1)
```

\
This plot also has some pattern and we see that even with transformation of the variable, we were not able to completely account for linearity.

Let's now check the structure of the data. Given ggpairs is intensive, we use only the significant predictors and response to plot the relationships.

```{r}
ggpairs(data=dvisits, columns=c("agesq","freerepa","actdays",
            "chcond2","nondocco","hospadmi","hospdays"), title="dvisits data")
```

Given the discrete values in most of the variables, it's difficult to asses linear relationship above. We can however see the positive correlations mostly with the response variable.

\newpage

### 6) The following data provides the Covid-19 cases per state since January. https://covidtracking.com/api/v1/states/daily.csv . The purpose is to predict the total number of cases in US per day with linear regression. Use data till Sept for training and rest for testing. Perform diagnostics and show why your model is good. 

Let us load the file.

```{r}
daily <-  read.csv('/Users/mac/Downloads/daily.csv')
str(daily)
```

Let's summarise this information -

```{r}
summary(daily)
```

### Key observations -

1. The data is at state level and the question wants us to come up with a model for "total number of cases per day in the US" so we need to roll up the data at a day-level. \
2. The date variable needs to be converted to date/ ts format. \
3. We see quite a few NAs in the data. \
4. The total column is the sum of positive, negative and pending cases which makes sense and will be our pendent variable. \
4. Our variables of interest seem to be date, state (we can take a count of state per day), positive, negative, probableCases, pending, totalTestResults, death, total. There is a reason for not choosing variables like hospitalizedCurrently or inIcuCumulative since we do not expect total cases per day to be influenced by such factors, however we do expect more total cases per day if there are more tests conducted. \
5. We have geographical identifier in 'FIPS' but we will later add latitude and longitude while making map plots.

### However do note that in our test time frame, we will of course not have such breakdowns of total cases present, hence we won't be able to use these explanatory variables for forecasting. So our modeling form will most likely be -

$y_t= \alpha * y_{t-1} + \beta * y_{t-2} + ... \epsilon$

### i.e our total number of cases per day will be regressed on previous day and significant lags in the past to predict for future. This is what Time Series Regression methods like Autoregression, ARIMA, SARIMA, and more complex models like LSTM (a type of neural network) can perform. 

### Data prep -

For now we limit our dataset to the following variables and construct a rolled up day level data -

```{r}
# subset the data for above variables
state_daily <- daily[,c("date","state","positive",
     "negative", "probableCases", "pending",
    "totalTestResults", "death", "total","totalTestResultsIncrease")]
str(state_daily)
```

We replace the NAs by 0 so we can sum them up

```{r}
state_daily[is.na(state_daily)] <- 0
str(state_daily)
```

### Roll up the data to day level -

```{r}
overall_daily <- state_daily %>%
  dplyr::group_by(date) %>% 
  dplyr::summarise(pos=sum(positive),neg=sum(negative),
                   probable = sum(probableCases),
                   pend = sum(pending),
                   totalTest = sum(totalTestResults),
                   totalTestInc = sum(totalTestResultsIncrease),
                   deaths = sum(death),
                   totals = sum(total)
                   )
```

We look at the most recent data and correlate it with the official numbers. There were ~15 Mn cases reported as of Dec 9th, 2020 in US and our pos column shows 15.2 Mn positive COVID-19 cases. In order to predict the Total COVID-19 cases per day, we want to accurately predict this "pos" column.

\newpage

### Visualise the Data 

We use maps for this now and aggregate the data at a state level.

```{r}
require(usmap)
require(openintro)
require(maps)

covidus <- daily
abb=covidus$state
# We convert the region names to full names with abbr2state
region=abbr2state(abb)


covidus2=cbind(covidus,region)
covidus2['fips'] <- fips(covidus2$state)
states=map_data("state")

####Aggregating the data ####

states2=states %>%
  dplyr::group_by(region) %>%
  dplyr::summarise(lat = max(lat),long=max(long))


covid_agg=covidus2 %>%
  dplyr::group_by(region,state) %>%
  dplyr::summarise(total_pos = sum(positive,na.rm = TRUE),
                   total_neg=sum(negative,na.rm = TRUE),
            total_nt=sum(pending,na.rm = TRUE),
            total_t=sum(total,na.rm = TRUE),
            total_deaths=sum(death,na.rm = TRUE)
            )
covid_agg$region=tolower(covid_agg$region)

map.df <- merge(covid_agg,states2, by="region", all.x = T)

```

### convert date to date format

```{r}
state_daily$date <- as.Date(as.character(state_daily$date),format="%Y%m%d")
#daily$date <- as.Date(as.character(daily$date),format="%Y%m%d")
covidus2$date <- as.Date(as.character(covidus2$date),format="%Y%m%d")
str(state_daily)
```

We create a map.df and now we look at US maps plot.

### Total positive cases by state in the US
```{r}
plot_usmap(data = map.df, values = "total_pos",   labels=TRUE) + 
  scale_fill_continuous( low = "white", high = "orange", 
                         name = "Positive Cases", label = scales::comma
  ) + 
  theme(legend.position = "right") + 
  theme(panel.background = element_rect(colour = "black")) + 
  labs(title = "Positive cases of Covid", caption = "Source: @SRK")
```

We see few states like California, Texas, New York, Florida have really high cases.

### Total pending cases by state in the US
```{r}
####total pending cases in US####
plot_usmap(data = map.df, values = "total_nt",   labels=TRUE) + 
  scale_fill_continuous( low = "white", high = "orange", 
                         name = "total_nt", label = scales::comma
  ) + 
  theme(legend.position = "right") + 
  theme(panel.background = element_rect(colour = "black")) + 
  labs(title = "Pending cases of Covid19 in US", caption = "Source: @SRK")

```

We ee very high pending cases in CA, OK and FL.

### Total deaths by state in the US
```{r}
####total non treated cases in US####
plot_usmap(data = map.df, values = "total_deaths",   labels=TRUE) + 
  scale_fill_continuous( low = "white", high = "orange", 
                         name = "total_deaths", label = scales::comma
  ) + 
  theme(legend.position = "right") + 
  theme(panel.background = element_rect(colour = "black")) + 
  labs(title = "Deaths in Covid19 in US", caption = "Source: @SRK")

```

### Positive trend of cases in 4 states - TX, CA, FL, NY

```{r}
### Subsetting the data for four states with high cases ####
sub_us=subset(covidus2 , subset =(covidus2$state %in% c('TX','CA','FL','NY')))

#### Infected patients growth in 4 states
p <- ggplot(data = sub_us, aes(x = date, y = positive),color= state)

p + geom_line(aes(group = state)) + facet_wrap(~ state)
```

We notice a not so sharp increase in NY in Jul, while in Florida and Texas between Apr-Jul, we see cases spiking slowly. This maybe due to limited testing at the time, and later months look quite grim.

### Death trend of cases in 4 states - TX, CA, FL, NY

```{r}
p <- ggplot(data = sub_us, aes(x = date, y = death))
p + geom_line(aes(group = state)) + facet_wrap(~ state)
```

Unfortunately, the number of deaths spiked in NY in the months of Apr-June and then stabilising. the other states mostly have a linear increase in death rate.

### Cumulative tests by day for all state

```{r}
###Tests By Day###
totaltest <- (aggregate(daily["totalTestResultsIncrease"]
  , by=daily["date"], sum, na.rm=TRUE, na.action=NULL))

ggplot(data=totaltest, aes(x=date, 
y=totalTestResultsIncrease)) +geom_point() +
  geom_smooth(fill=NA, size=0.5) + labs(x = "Dates") +
ggtitle("Cum tests by day for all states") +
  scale_y_continuous(name="Total Tests By Day", labels = scales::comma)
```

We see that overall cumulative trend has continuously gone up.

### Total tests by date separated by State 
```{r}
ggplot(data=daily, aes(x=date, 
  y=totalTestResults, colour = state)) +geom_point() +
  geom_smooth(fill=NA, size=0.5) + labs(x = "Dates") + 
  ggtitle("Total tests by date separated by State") +
  scale_y_continuous(name="Total Tests By Day", labels = scales::comma)
```

We can further look at more visuals however we feel we can now proceed to modeling the question.

\newpage

Let's start by building a time series model with 'pos' - total number of positive cases daily in US in our overall_daily dataset.

For time series regression, we will first perform some checks and then build an ARIMA model folowed by ARIMAX. The major factors that may influence the covid cases, like income, gdp, education level, social distancing practice, are not in this particular dataset so we do not incorporate those.

### Converting our rolled up data to date format

```{r}
overall_daily$date <- as.Date(as.character(overall_daily$date),format="%Y%m%d")
str(overall_daily)
```

We prepare our data as below -
```{r}
#creating additional variables
data <- overall_daily %>%
         dplyr::mutate(
         Date=as.Date(date, format="%Y%m%d"),
         day_of_week = wday(Date, label = TRUE),
         day_of_week_index = wday(Date),
         wk_of_month=ceiling(day(Date) / 7 ),
         month = format(Date, "%m"))
```

### Removing the rows with 0 cases

```{r}
data <- filter(data, pos != 0)
```


### Visualizing the pos (covid positive cases) series

```{r Closing Price, echo=FALSE}

#plot normal closing series
plot(data$pos,xlab='time',ylab='pos',type='l',col='blue')

abline(reg=lm(data$pos~time(data$Date))) # fit a trend line

```

### Seasonality (monthly) check for 1 year of daily data

We now check if monthly averages over 1 years of data show any seasonality or pattern by month
i.e are there months wherein covid cases rose and fell ?

```{r}
#do monthly averages show seasonality ?
seasonalchk <- data %>% dplyr::group_by(month) %>%
  dplyr::summarize(mean_pos=mean(pos)) %>%
  dplyr::group_by(month) %>%
  dplyr::summarize(avg_pos=mean(mean_pos))

head(seasonalchk)

#plot the series of mean prices by month
plot(seasonalchk$avg_pos,xlab="month",ylab="covidavg",type='l',col='blue')

```

We ee that cases didn't start increasing until month 3 - March but then took a huge turn (almost exponential increase).

### splitting into train and test

```{r}
train <- filter(data,month %in% c("01","02","03","04","05",
                            "06","07","08","09"))
test <- filter(data,month %in% c("10","11","12"))

str(train)
str(test)
```


### Converting series to a ts object

In order to analyze a time series in R, we need to convert the data frame as a ts (time series) object. The R language uses many functions to create, manipulate and plot the time series data. The data for the time series is stored in an R object called time-series object. It is also an R data object like a vector or data frame. The time series object is created by using the ts() function and is easier to manipulate post conversion.

```{r}

#is the positive cases field a ts object or not
print(is.ts(data$pos))

#converting to ts object
train.ts <- ts(train$pos,frequency = 30)
test.ts <- ts(test$pos,frequency = 30)
print(is.ts(train.ts))
frequency(train.ts)

```

Our time series training set has 215 observations from Jan-Sep while our test set has 73 observations from Oct-Dec.

### Series Decomposition

Time series decomposition is a mathematical procedure which transforms a time series into multiple different time series. The original time series is often split into 3 component series:

1. Seasonal: Patterns that repeat with a fixed period of time. For example, a website might receive more visits during weekends; this would produce data with a seasonality of 7 days. \
2. Trend: The underlying trend of the metrics. A website increasing in popularity should show a general trend that goes up. \
3. Random: Also call noise, irregular or remainder, this is the residuals of the original time series after the seasonal and trend series are removed. \

We will try and decompose the covid positive cases using the stl() function in R.

### Series Decomposition with stl()

```{r}
#decompose covid cases into trend, seasonal and remainder using stl
dcomposeclose=stl(train.ts, "periodic")
plot(dcomposeclose)
```

We see a clear trend and seasonal pattern in the data.

### Testing the series for stationarity, autocorrelation and normality

We now want to analyze the covid series for properties that make forecasting simpler and give us more insight into its nature. 

1. Stationarity - A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., "stationarized") through the use of mathematical transformations. A stationarized series is relatively easy to predict. We use adf.test (Augmented Dickey Fuller Test) function in R to check whether a series is stationary or not. This function computes the augmented Dickey-Fuller statistic for testing the null hypothesis that there exists a unit root at the zero frequency. \

2. Autocorrelation - The Box Ljung test will be used to determine if the residuals are independent or not. We will use the Box.test function in R for this. \

3. Normality - can be tested by the Jarque-Bera Test. The test statistic is always nonnegative. If it is far from zero, it signals the data does not have a normal distribution. We will use jarque.bera.test function in R for this. \

### Stationarity 

```{r}
#testing stationarity for covid
adf.test(train.ts)
```

We see a p-value of 0.01 which indicates that we can reject the null and conclude that the series is stationary.

### Autocorrelation

```{r}
#box jung test
Box.test(train.ts,type='Ljung')
```

We have significant p-value indicating the presence of autocorrelation in residuals.

### normality

```{r}
#jarque bera test
tseries::jarque.bera.test(train.ts)
```

&nbsp;

We again have significant p-values indicating the presence of non-normal covid cases.

\newpage

# First iteration - ARIMA model 

ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. It is a class of model that captures a suite of different standard temporal structures in time series data. This acronym is descriptive, capturing the key aspects of the model itself. Briefly, they are:

1. AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations. \
2. I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary. \
3. MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. \

Each of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.

The parameters of the ARIMA model are defined as follows:

1. p: The number of lag observations included in the model, also called the lag order. \
2. d: The number of times that the raw observations are differenced, also called the degree of differencing. \
3. q: The size of the moving average window, also called the order of moving average. 

A key note here is that adopting an ARIMA model for a time series assumes that the underlying process that generated the observations is an ARIMA process. Whether this is true or not, can be found out once we look at the results of the modeling process.

### ARIMA model form

We use auto.arima () for this 

```{r}
#arima to check order for covid cases for last year
fit_diff_covid <-auto.arima(train.ts)
fit_diff_covid

```

We get a arima model of the form ARIMA(2,2,2) indicating difference of 2 AR of 2 and MA of 2. Our AIC is 4137.

## Visually understanding the AR and MA terms through ACF and PACF plots

```{r}
#acf and pacf plots for covid series
acf(train.ts)
pacf(train.ts)
```

We don't see any meaningful insight by analyzing covid series. However, the significant lag at 1 in PACF plot and all lags significant in ACF indicate that autocorrelation from lag 2 to n is propogated by the autocorrelation at lag 1.

## Estimating Final ARIMA model selected on AIC

```{r}
predicted <- forecast::forecast(fit_diff_covid,h=73)
predicted
plot(predicted,col = 'red')
```

## Computing accuracy of the model

We compute MAPE to check accuracy of the arima model.

```{r}
set.seed(7)

prediction_test <- as.data.frame(predicted)

comparison <- as.data.frame(cbind(prediction_test$`Point Forecast`,test$pos))
comparison$mape <- (abs(comparison$V2-comparison$V1)/abs(comparison$V2))
summary(comparison$mape)
```

### We get an accuracy median MAPE of 9.6% and average MAPE of 12.9% which is decent for a base ARIMA model.

We plot the predicted vs actual plot for the test set.

```{r}
ggplot(comparison,aes(x = prediction_test$`Point Forecast`, y = test$pos) ) +
     geom_point() +
     geom_smooth(method = "lm", se = FALSE)
```

\newpage 

### We now explore ARIMAX

ARIMAX is basically ARIMA but with an x variable - our key hypothesis is that the total positive cases per day (detected) can be influenced by testing i.e more the tests conducted, more will be the detected positive cases per day. We use the totalTest and totalTestInc variables as our explantory variable in this iteration

We first put these variables into a matrix form -

```{r}
totaltest_mat <- as.matrix(train$totalTest,train$totalTestInc)
```

and then pass it in the xreg parameter -

```{r}
#arima to check order for covid cases for last year
fit_diff_covid <- auto.arima(train.ts,xreg = totaltest_mat)
fit_diff_covid
```

We see a model of form 2,2,2 with AIC of 4127.8

```{r}
#estimation
arima.final <- arima(train.ts,c(2,2,2))
```

```{r}
predicted <- forecast::forecast(arima.final,h=73)
predicted
plot(predicted,col = 'red')
```

## Computing accuracy of the model

We compute MAPE to check accuracy of the arima model.

```{r}
set.seed(7)

prediction_test <- as.data.frame(predicted)

comparison <- as.data.frame(cbind(prediction_test$`Point Forecast`,test$pos))
comparison$mape <- (abs(comparison$V2-comparison$V1)/abs(comparison$V2))
summary(comparison$mape)
```

### We get an accuracy median MAPE of 9.6% and average MAPE of 12.9% which is decent for a base ARIMAX model with two variables.

We plot the predicted vs actual once again for this model.

```{r}
ggplot(comparison,aes(x = prediction_test$`Point Forecast`, y = test$pos) ) +
     geom_point() +
     geom_smooth(method = "lm", se = FALSE)
```

We don't see much improvement in adding the test variables to ARIMA.

We feel given the seasonality is unknown since we have less than a year of data, models like ARIMA and ARIMAX are not able to forecast with a much higher precision. 

### Future Scope - We also know about the COVID-19 Open Research Dataset Challenge (CORD-19)( https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge). This was created in response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 51,000 scholarly articles, including over 40,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. This dataset is ideal for performing Text mining. Also besides this dataset, offered by US CDC, we can can explore if there are any significant factors such as income, gdp, and education level that may potentially affect COVID 19 outbreak. This can be extracted as well and we feel utilizing the above 3 datasets, we can have not only a good prediction of total cases per day, but also about what factors influence those cases. We can also try an LSTM model for this problem.


\newpage

# Appendix -

We tried some additional modeling methods and got the following results for COVID prediction however we did not include these methods as the MAPE wasn't as great as ARIMA models. We tried naive, ETS, TBATS models in R. Feel free to peruse.

###  Naive forecast

```{r}
naive = snaive(train$pos, h=length(test$pos))
MAPE(naive$mean, test$pos) * 100
```

```{r}
plot(data$pos, col="blue", xlab="Date", ylab="Covid Cases", main="Seasonal Naive Forecast", type='l')
lines(naive$mean, col="red", lwd=2)
```

### ETS - Exponential models

```{r}
ets_model = ets(train$pos, allow.multiplicative.trend = TRUE)
summary(ets_model)
```

```{r}
ets_forecast = forecast::forecast(ets_model, h=length(test$pos))
MAPE(ets_forecast$mean, test$pos) *100
plot(data$pos, col="blue", xlab="Date", ylab="Covid Cases", main="ETS Forecast", type='l')
lines(ets_forecast$mean, col="red", lwd=2)
```

### TBATS forecast

```{r}
tbats_model = tbats(train$pos)
tbats_forecast = forecast::forecast(tbats_model, h=length(test$pos))
MAPE(tbats_forecast$mean, test$pos) * 100

plot(data$pos, col="blue", xlab="Date", ylab="Covid Cases", main="TBATS Forecast", type='l')
lines(tbats_forecast$mean, col="red", lwd=2)
```
